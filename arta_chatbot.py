# -*- coding: utf-8 -*-
"""ARTA_ChatBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uOXl9w-lij6C-Tl62osQYGjlQXMhzK_q
"""

!pip install langchain langchain_community sentence-transformers transformers faiss-gpu python-dotenv

from google.colab import drive
drive.mount('/content/drive')

"""#Load and test Indexing & Retrieval"""

import sqlite3

# Path to the SQLite database
db_path = '/content/drive/My Drive/PDFs_Capstone/LitRev_v2.db'

# Open the database connection
conn = sqlite3.connect(db_path)
cur = conn.cursor()

# Execute a query to count the number of records
cur.execute('SELECT COUNT(*) FROM json_data')
record_count = cur.fetchone()[0]

# Close the database connection
conn.close()

print(f"Number of records in the json_data table: {record_count}")

import sqlite3

# Function to retrieve data from SQLite based on category_number and document_number
def retrieve_data_by_category_and_document(category_number, document_number):
    conn = sqlite3.connect(db_path)
    cur = conn.cursor()

    # Execute a query to fetch the data
    cur.execute('''
        SELECT * FROM json_data
        WHERE category_number = ? AND document_number = ?
    ''', (category_number, document_number))

    results = cur.fetchall()
    conn.close()

    return results

# Example usage
category_number = "Cat-11"
document_number = "Doc (3)"
data = retrieve_data_by_category_and_document(category_number, document_number)

# Print the retrieved data
for record in data:
    print(record)

import sqlite3
import numpy as np
import pickle
import faiss
from langchain_community.embeddings import HuggingFaceEmbeddings

# Path to the SQLite database and embeddings file
db_path = '/content/drive/My Drive/PDFs_Capstone/LitRev_v2.db'
embeddings_path = '/content/drive/My Drive/PDFs_Capstone/embeddings_v3.pkl' #faiss db

# Initialize the embedding model
embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')

# Load FAISS index and embeddings from file
index = faiss.IndexFlatL2(768)  # initializes an index that uses L2 distance to measure similarity between two indexes and it also expects
#the vector dimension to be 768 which is the same as the embedding model in this case would produce.
embeddings = []
identifiers = [] #index positions? check by loading the identifiers maybe?

# Load embeddings and identifiers from pickle file
with open(embeddings_path, 'rb') as f:
    while True:
        try:
            embedding, identifier = pickle.load(f)
            embeddings.append(embedding)
            identifiers.append(identifier)
        except EOFError:
            break

# Convert embeddings to numpy array and add to FAISS index
embeddings_array = np.array(embeddings, dtype='float32')
index.add(embeddings_array)

#Give meta data, get text chinks from sqlite
def retrieve_chunk_text(category_number, document_number, page_number, section_number, chunk_number):
    conn = sqlite3.connect(db_path)
    cur = conn.cursor()
    cur.execute('''
        SELECT chunk_text FROM json_data
        WHERE category_number = ? AND document_number = ? AND page_number = ? AND section_number = ? AND chunk_number = ?
    ''', (category_number, document_number, page_number, section_number, chunk_number))
    result = cur.fetchone()
    conn.close()
    if result:
        return result[0]
    else:
        return None

# Function to retrieve embeddings using FAISS. identifiers of top 5 chunks are returned
def retrieve_embeddings(query_embedding, k=5):
    query_embedding = np.array(query_embedding).astype('float32')
    query_embedding = query_embedding.reshape(1, -1)  # Reshape for FAISS

    # Search in FAISS index
    distances, indices = index.search(query_embedding, k)

    # Retrieve corresponding identifiers
    retrieved_identifiers = [identifiers[i] for i in indices[0]]

    return retrieved_identifiers #as a result of search , we retrieved the identifiers of the top 5 chunks that are closer to the querry embedding.

def retrieve_and_print_chunks(query, k=5):
    # Embed the query
    query_embedding = embedding_model.embed_query(query)
    query_embedding = np.array(query_embedding, dtype='float32')

    # Retrieve identifiers using FAISS
    retrieved_identifiers = retrieve_embeddings(query_embedding, k)
    print(retrieved_identifiers)
    # Fetch corresponding chunk texts from SQLite
    retrieved_text = []
    for cat_num, doc_num, page_num, sec_num, chunk_num in retrieved_identifiers:
        text_chunk = retrieve_chunk_text(cat_num, doc_num, page_num, sec_num, chunk_num)
        if text_chunk:
            retrieved_text.append(text_chunk)

    return retrieved_text

# Example usage:
query = "Intended use of enviroment for ARTSENS Plus device or ARTSENS series"
retrieved_chunks = retrieve_and_print_chunks(query)
for idx, chunk in enumerate(retrieved_chunks, start=1):
    print(f"Chunk {idx}: {chunk}\n")

"""#LLM integration : Gemini finalised!"""

#@title T5 model : Do not use this!
from transformers import T5Tokenizer, T5ForConditionalGeneration
t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')
t5_model = T5ForConditionalGeneration.from_pretrained('t5-large')
def generate_response_t5(query):
    # Retrieve relevant documents
    retrieved_docs = retrieve_and_print_chunks(query)
    context = " ".join(retrieved_docs)

    # Prepare input for T5/prompt
    #input_text = f"Please answer the following question based on the given context.\n\nQuestion: {query}\n\nContext: {context}"

    input_text = (
    f"Answer the question based on the given context.\n\n"
    f"Context: {context}\n\n"
    f"Question: {query}"
)
    input_ids = t5_tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True) #for the model to interpret question

    # Generate response
    outputs = t5_model.generate(
        input_ids,
        max_length=150,  # Increased max length
        min_length=50,  # Set a minimum length
        num_beams=5,  # Beam search for better quality
        early_stopping=True,
        no_repeat_ngram_size=2,  # Prevent repetition
        #temperature=0.7,  # Control the creativity of the responses.higher the more.
        #top_k=50,  # Limit the sampling pool
    )
    answer = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)

    return answer

generate_response_t5("What is early vascular ageing")

#@title Gemini model: currently used for final Application
from google.colab import userdata
import google.generativeai as genai
import os
import dotenv
dotenv.load_dotenv("/content/drive/My Drive/PDFs_Capstone/.env")

genai.configure(api_key=os.environ['API_KEY'])

gemini_model = genai.GenerativeModel(
    'gemini-1.5-flash',
    generation_config=genai.GenerationConfig(
      temperature=0.1,
      max_output_tokens=100
    ),
    system_instruction = "\
  You are a helpful and informative bot called Arta that answers questions using text from the reference passage included below. \
  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \
  However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \
  strike a friendly and converstional tone. \
  If the passage is irrelevant to the answer, ignore it."
)

def generate_response_gemini(query: str, history: list[str] = None): #the function takes a query as well as history of paast interactions
    if history is None: history = []

    retrieved_docs = retrieve_and_print_chunks(query)
    context = " ".join(retrieved_docs)
    escaped = context.replace("'", "").replace('"', "").replace("\n", " ")
    prompt = ("""\
  QUESTION: '{query}'
  PASSAGE: '{relevant_passage}'
  """).format(query=query, relevant_passage=escaped)
    resp = gemini_model.generate_content([
        {'role': 'user' if (i%2 == 0) else 'model', 'parts': [history[i]]} for i in range(len(history))
    ] + [{'role': 'user', 'parts': [prompt]}])
    return resp.text.strip()

# Dummy test response
generate_response_gemini("How can I measure it?", ["What is vascular age", "Vascular age is a measure of how old your blood vessels are, regardless of your actual age."])

"""#ARTA - Flask based application"""

from google.colab.output import eval_js

print('CLICK THE LINK BELOW')
print(eval_js("google.colab.kernel.proxyPort(5000)"))
print('\n\n')

#-----------------FLASK-------------------------
from typing import Any
from flask import Flask, send_from_directory, request
import sys
import json

# creates a Flask application
STATIC_FOLDER = "/content/drive/My Drive/PDFs_Capstone/static"
app = Flask(__name__, static_folder=STATIC_FOLDER)

@app.get('/static/<path:path>')
def send_file(path):
    """Handles serving static files (like the image, css, javascript and html files)"""
    return send_from_directory(STATIC_FOLDER, path)

@app.route("/")
def home():
    """Basically serves static/index.html"""
    return send_from_directory(STATIC_FOLDER, 'index.html')


@app.post("/query")
def resolve_query():
    """Reesolves query sent by the user :)"""
    query = request.get_data(as_text=True)
    try:
        qobj = json.loads(query)
        return get_answer(qobj['query'], qobj['history'])
    except Exception as e:
        return f"ERROR! Could not handle query due to `{e}`"


def get_answer(query: str, history: list[str]):
    """Function to answer the query :)"""
    return generate_response_gemini(query, history)

app.run()